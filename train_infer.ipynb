{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772e693-2042-43c7-8de7-b167ab58272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20468d3-f35a-46df-9751-fa488e05e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据组装函数\n",
    "def assemble(day):\n",
    "    '''\n",
    "    day: '20200320'\n",
    "    '''\n",
    "    year = day[:4]\n",
    "    # z(geopotential)\n",
    "    file_path = f'pressure_level/geopotential/{year}/{day}.nc'\n",
    "    file_obj = nc.Dataset(file_path)\n",
    "    z = file_obj.variables['z']\n",
    "    z = z[:]\n",
    "    # t(temperature)\n",
    "    file_path = f'pressure_level/temperature/{year}/{day}.nc'\n",
    "    file_obj = nc.Dataset(file_path)\n",
    "    t = file_obj.variables['t']\n",
    "    t = t[:]\n",
    "    # u(u_component_of_wind)\n",
    "    file_path = f'pressure_level/u_component_of_wind/{year}/{day}.nc'\n",
    "    file_obj = nc.Dataset(file_path)\n",
    "    u = file_obj.variables['u']\n",
    "    u = u[:]\n",
    "    # v(v_component_of_wind)\n",
    "    file_path = f'pressure_level/v_component_of_wind/{year}/{day}.nc'\n",
    "    file_obj = nc.Dataset(file_path)\n",
    "    v = file_obj.variables['v']\n",
    "    v = v[:]\n",
    "    # q(specific_humidity)\n",
    "    file_path = f'pressure_level/specific_humidity/{year}/{day}.nc'\n",
    "    file_obj = nc.Dataset(file_path)\n",
    "    q = file_obj.variables['q']\n",
    "    q = q[:]\n",
    "    # ciwc(specific_cloud_ice_water_content)\n",
    "    file_path = f'pressure_level/specific_cloud_ice_water_content/{year}/{day}.nc'\n",
    "    file_obj = nc.Dataset(file_path)\n",
    "    ciwc = file_obj.variables['ciwc']\n",
    "    ciwc = ciwc[:]\n",
    "    # clwc(specific_cloud_liquid_water_content)\n",
    "    file_path = f'pressure_level/specific_cloud_liquid_water_content/{year}/{day}.nc'\n",
    "    file_obj = nc.Dataset(file_path)\n",
    "    clwc = file_obj.variables['clwc']\n",
    "    clwc = clwc[:]\n",
    "    # crwc(specific_rain_water_content)\n",
    "    file_path = f'pressure_level/specific_rain_water_content/{year}/{day}.nc'\n",
    "    file_obj = nc.Dataset(file_path)\n",
    "    crwc = file_obj.variables['crwc']\n",
    "    crwc = crwc[:]\n",
    "    # cswc(specific_snow_water_content)\n",
    "    file_path = f'pressure_level/specific_snow_water_content/{year}/{day}.nc'\n",
    "    file_obj = nc.Dataset(file_path)\n",
    "    cswc = file_obj.variables['cswc']\n",
    "    cswc = cswc[:]\n",
    "    # assemble\n",
    "    arr = np.concatenate((z.data[:, :, 35:81, 70:141], \n",
    "                          t.data[:, :, 35:81, 70:141], \n",
    "                          u.data[:, :, 35:81, 70:141], \n",
    "                          v.data[:, :, 35:81, 70:141], \n",
    "                          q.data[:, :, 35:81, 70:141],\n",
    "                          ciwc.data[:, :, 35:81, 70:141], \n",
    "                          clwc.data[:, :, 35:81, 70:141], \n",
    "                          crwc.data[:, :, 35:81, 70:141], \n",
    "                          cswc.data[:, :, 35:81, 70:141]), \n",
    "                         axis=1)\n",
    "    # check sanity\n",
    "    assert arr.shape == (4, 117, 46, 71)\n",
    "    return arr\n",
    "\n",
    "\n",
    "os.makedirs('prepared_data', exist_ok=True)\n",
    "all_days = sorted([i.split('/')[-1].replace('.nc', '') for i in glob.glob('pressure_level/geopotential/*/*.nc')])\n",
    "with Pool(8) as pool:\n",
    "    arr = list(\n",
    "        tqdm(\n",
    "            pool.imap(assemble, all_days),\n",
    "            total=len(all_days),\n",
    "            desc=\"Generating data\",\n",
    "        )\n",
    "    )\n",
    "arr = np.concatenate(arr)\n",
    "print('processed data shape:', arr.shape)\n",
    "np.save('prepared_data/data', arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3116e8-4b77-4bd0-8ccf-7ea457ee61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x 归一化 (nunique==1 的特征不用了)\n",
    "x_indics = [i for i in list(range(117)) if i not in [78,79,80,91,92,93]]\n",
    "x_arr = arr[:, x_indics, :, :]\n",
    "x_mean = x_arr.mean(axis=(0, 2, 3), keepdims=True)\n",
    "x_std = x_arr.std(axis=(0, 2, 3), keepdims=True)\n",
    "np.save('prepared_data/norm_mean_x', x_mean)\n",
    "np.save('prepared_data/norm_std_x', x_std)\n",
    "\n",
    "# y 归一化\n",
    "y_indics = [16,20,22,23,25,55,59,61,62,64,68,72,74,75,77,81,85,87,88,90,94,98,100,101,103,107,111,113,114,116]\n",
    "y_arr = arr[:, y_indics, :, :]\n",
    "y_mean = y_arr.mean(axis=(0, 2, 3), keepdims=True)\n",
    "y_std = y_arr.std(axis=(0, 2, 3), keepdims=True)\n",
    "np.save('prepared_data/norm_mean_y', y_mean)\n",
    "np.save('prepared_data/norm_std_y', y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38449578-84f6-46dc-89f3-3d12be042c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
zhengheng@my-ubuntu:~/competitions/sais2025_baseline$ cat train_infer.ipynb
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88062bf-dd55-44db-81a0-b40d75359d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "def format_time(elapsed):\n",
    "    \"\"\"Take a time in seconds and return a string hh:mm:ss.\"\"\"\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "def seed_everything(seed_val=42):\n",
    "    \"\"\"Seed everything.\"\"\"\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "seed_everything(1024)\n",
    "\n",
    "XCOLS = [i for i in list(range(117)) if i not in [78,79,80,91,92,93]]\n",
    "TARGETS = [16,20,22,23,25,55,59,61,62,64,68,72,74,75,77,81,85,87,88,90,94,98,100,101,103,107,111,113,114,116]\n",
    "N_TRAIN_SAMPLES = 7286\n",
    "DEVICE = 'cuda'\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "LR = 2e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5763d3-d419-4610-b287-431f986d2edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('prepared_data/data.npy')\n",
    "norm_mean_x = np.load('prepared_data/norm_mean_x.npy')\n",
    "norm_std_x = np.load('prepared_data/norm_std_x.npy')\n",
    "norm_mean_y = np.load('prepared_data/norm_mean_y.npy')\n",
    "norm_std_y = np.load('prepared_data/norm_std_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57166f2c-5435-48df-b7f8-f8de0c9f6486",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, indics, data):\n",
    "        self.indics = indics\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indics)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+2, XCOLS, :, :]\n",
    "        y = self.data[index+2:index+14, TARGETS, :, :]\n",
    "        x = (x - norm_mean_x) / norm_std_x\n",
    "        y = (y - norm_mean_y) / norm_std_y\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        return x, y\n",
    "\n",
    "\n",
    "train_dataset = NumpyDataset(list(range(N_TRAIN_SAMPLES)), data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c8a18-7826-43f7-8c38-43ee11f449b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(卷积 => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"下采样模块\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"上采样模块\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # 调整尺寸差异\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        \n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels=222, output_channels=360):\n",
    "        super(UNet, self).__init__()\n",
    "        # 编码器\n",
    "        self.inc = DoubleConv(input_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        \n",
    "        # 解码器\n",
    "        self.up1 = Up(1024+512, 512)\n",
    "        self.up2 = Up(512+256, 256)\n",
    "        self.up3 = Up(256+128, 128)\n",
    "        self.up4 = Up(128+64, 64)\n",
    "        \n",
    "        # 最终输出层\n",
    "        self.outc = nn.Conv2d(64, output_channels, kernel_size=1)\n",
    "        \n",
    "        # 时间特征融合\n",
    "        self.time_proj = nn.Sequential(\n",
    "            nn.Linear(2, 64),  # 2个初始时间点\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256)\n",
    "        )\n",
    "        \n",
    "        # 特征维度适配\n",
    "        self.feat_proj = nn.Conv2d(111, 30, 1)  # 111维特征到30维的投影\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 原始输入形状: (batch, 2, 111, 46, 71)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 合并时间和特征维度\n",
    "        x = x.view(batch_size, 2*111, 46, 71)\n",
    "        \n",
    "        # 编码器路径\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        # 解码器路径\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "        # 最终输出\n",
    "        logits = self.outc(x)\n",
    "        \n",
    "        # 调整输出维度\n",
    "        logits = logits.view(batch_size, 12, 30, 46, 71)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba069cc-e03f-4842-a94d-912c8061639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "num_train_steps = int(N_TRAIN_SAMPLES / BATCH_SIZE * EPOCHS)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=0, num_training_steps=num_train_steps, num_cycles=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e74c75-2641-420f-b46a-24344cbb21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "        x = x.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = nn.SmoothL1Loss(reduction='mean')(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "        if (batch_idx + 1) % 200 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            elapsed_time = format_time(time.time() - ts)\n",
    "            print(f'  Epoch: {epoch+1}',\\\n",
    "                  f'  Batch: {batch_idx + 1}/{len(train_loader)}',\\\n",
    "                  f'  Train Loss: {total_loss / steps:.6f}',\\\n",
    "                  f'  LR: {current_lr:.3e}',\\\n",
    "                  f'  Time: {elapsed_time}', flush=True)\n",
    "            total_loss = 0\n",
    "            steps = 0\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20316bb7-e353-4929-8278-5827bd02a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "os.makedirs('output', exist_ok=True)\n",
    "for i in tqdm(range(60)):\n",
    "    file_path = f'input/{i:03d}.pt'\n",
    "    test_input = torch.load(file_path, weights_only=True)\n",
    "    test_input = test_input[:, :, XCOLS, 35:81, 70:141]\n",
    "    test_input = (test_input - norm_mean_x) / norm_std_x\n",
    "    pred = model(test_input.to(DEVICE))\n",
    "    out = pred.detach().cpu().numpy() * norm_std_y + norm_mean_y\n",
    "    out = torch.from_numpy(out).to(torch.float16)\n",
    "    torch.save(out, f'output/{i:03d}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e75ab-19f4-424d-841e-76f34e2ff230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
